## Training Hyperparameters
seq_length: 512  # 32768 | We are not retraining the embedding layers
inf_max_tokens: 128
batch_size: 4
lora_rank: 4
hidden_layers: 8  # 24 is the trained-one forward | We are not retraining the hidden layers
model_size: "0.5b"
training_steps: 10
learning_rate: 0.001  # 1e-5
forecast_length: 10
max_tokens: 512  # Same as seq_length

train_split: 0.03
time_step_split: 0.8
