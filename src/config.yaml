## Training Hyperparameters
seq_length: 512  # 32768 | We are not retraining the embedding layers
inf_max_tokens: 128
batch_size: 8
lora_rank: 4
hidden_layers: 8  # 24 is the trained-one forward | We are not retraining the hidden layers
model_size: "0.5b"
training_steps: 1000
learning_rate: 0.001  # 1e-5
forecast_length: 5
max_tokens: 512  # Same as seq_length
val_limit : 1

train_split: 0.8
time_step_split: 0.8
