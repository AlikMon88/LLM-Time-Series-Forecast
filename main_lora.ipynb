{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am3353/am3353/m2-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import time\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from src.qwen import load_qwen\n",
    "import re\n",
    "import yaml\n",
    "import gc\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device-activated:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print('Device-activated: ', device)\n",
    "file_path = \"data/lotka_volterra_data.h5\"  # Change this to the correct path  Out [9]:  Device-activated:  cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              76\n",
      "On-line CPU(s) list: 0-75\n",
      "Thread(s) per core:  1\n",
      "Core(s) per socket:  38\n",
      "Socket(s):           2\n",
      "NUMA node(s):        2\n",
      "Vendor ID:           GenuineIntel\n",
      "CPU family:          6\n",
      "Model:               106\n",
      "Model name:          Intel(R) Xeon(R) Platinum 8368Q CPU @ 2.60GHz\n",
      "Stepping:            6\n",
      "CPU MHz:             800.000\n",
      "CPU max MHz:         3700.0000\n",
      "CPU min MHz:         800.0000\n",
      "BogoMIPS:            5200.00\n",
      "L1d cache:           48K\n",
      "L1i cache:           32K\n",
      "L2 cache:            1280K\n",
      "L3 cache:            58368K\n",
      "NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74\n",
      "NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import src.data_prepare\n",
    "import src.forecast \n",
    "import src.preprocess\n",
    "import src.lora \n",
    "import src.data_create\n",
    "\n",
    "importlib.reload(src)\n",
    "importlib.reload(src.forecast)\n",
    "importlib.reload(src.preprocess)\n",
    "importlib.reload(src.lora)\n",
    "importlib.reload(src.data_create)\n",
    "importlib.reload(src.data_prepare)\n",
    "\n",
    "from src.forecast import *\n",
    "from src.lora import LoRALinear\n",
    "from src.preprocess import *\n",
    "from src.data_create import *\n",
    "from src.data_prepare import *  \n",
    "\n",
    "np.random.seed(random_state)     \n",
    "\n",
    "ft = time.time()\n",
    "model_lora, tokenizer = load_qwen()\n",
    "lt = time.time()\n",
    "\n",
    "print('time-taken: ', (lt - ft)/60, 'mins') \n",
    "\n",
    "pprint(model_lora.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can directly load modified model --> LoRATrainer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "manual_config = load_config(\"src/config.yaml\")  \n",
    "train_split = manual_config['train_split']\n",
    "time_step_split = manual_config['time_step_split']\n",
    "batch_size = manual_config['batch_size']\n",
    "learning_rate = manual_config['learning_rate'] # 1e-5\n",
    "lora_rank = manual_config['lora_rank']\n",
    "max_ctx_length = manual_config['seq_length']\n",
    "forecast_length = manual_config['forecast_length']\n",
    "max_tokens = manual_config['seq_length']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In [16]:  ### NO: test-train split because of chunking later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prey, data_prey_true, data_pred, data_pred_true, time_data_past, time_data_true = load_data(file_path, time_step_split, is_plot = True)\n",
    "print(data_prey.shape, data_prey_true.shape, data_pred.shape, data_pred_true.shape, time_data_past.shape, time_data_true.shape)  Out [16]:  Keys in HDF5 file: ['time', 'trajectories']\n",
    "\n",
    "check_rn = random.randint(0, len(data_prey))\n",
    "print('check_rn: ', check_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.plot(time_data_past, data_prey[check_rn], label = 'prey')\n",
    "plt.plot(time_data_past, data_pred[check_rn], label = 'predator')\n",
    "\n",
    "plt.plot(time_data_true, data_prey_true[check_rn], label = 'prey_truth', marker = '.')\n",
    "plt.plot(time_data_true, data_pred_true[check_rn], label = 'predator_truth', marker = '.')\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('population')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Prey-Predator Evolution | idx: {check_rn}')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lora.config.max_position_embeddings = manual_config['seq_length']\n",
    "model_lora.config.num_hidden_layers = manual_config['hidden_layers']  Dataset-Creation (Can't use the Untrained Qwen Preprocessing Module --> Because this one does a decoding-only chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_target_ids, val_input_ids, val_target_ids, prey_os, pred_os, test_encoded = prepare_data(data_prey, data_pred, tokenizer, max_ctx_length, train_split, forecast_length=forecast_length, is_forecast=True)\n",
    "print(train_input_ids.shape, train_target_ids.shape, val_input_ids.shape, val_target_ids.shape, test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_target_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_target_ids)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)  In [30]:  for batch_t, batch_v in zip(train_loader, val_loader):\n",
    "    print(batch_t[0].shape, batch_v[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA pre-training (we low-rank train the query and value projection matrices - retrain the attention networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model:\n",
    "### replacing attention layers with trainable layers (r * in_dim) + (out_dim * r)\n",
    "for layer in model_lora.model.layers:\n",
    "    layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=manual_config['lora_rank']) \n",
    "    layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=manual_config['lora_rank'])  In [32]:  optimizer = torch.optim.Adam((p for p in model_lora.parameters() if p.requires_grad), lr=learning_rate)  In [33]:  def get_model_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total, trainable = get_model_params(model_lora)\n",
    "print(f\"Total Parameters: {total:,}\")\n",
    "print(f\"Trainable Parameters: {trainable:,}\") ## 100x lesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Joint-Training\n",
    "Do Model Checkpointing for Large Optimization Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator()\n",
    "# model_lora, optimizer, train_loader, val_loader = accelerator.prepare(model_lora, optimizer, train_loader, val_loader)\n",
    "\n",
    "model_lora.train()\n",
    "\n",
    "target_steps = manual_config['training_steps']  # Optimization Steps\n",
    "print('Target-Train-Steps:', target_steps)\n",
    "\n",
    "train_steps = 0\n",
    "progress_bar = tqdm(range(target_steps), desc=\"Training Steps\")\n",
    "\n",
    "train_curve, val_curve = [], []\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# checkpoint_freq = 5  # Save model every 5 steps - adjust this as needed\n",
    "\n",
    "# # Create checkpoint directory if it doesn't exist\n",
    "# checkpoint_dir = \"model_checkpoints\"\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "ft = time.time()\n",
    "\n",
    "while train_steps < target_steps:\n",
    "    for batch_input_ids, batch_target_ids in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lora(batch_input_ids, labels=batch_target_ids)  # Use target_ids\n",
    "        loss = outputs.loss  # Loss function is a model attribute\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_curve.append(loss.detach().cpu().item())  # Store loss for monitoring\n",
    "\n",
    "        train_steps += 1\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # # Save checkpoint based on frequency\n",
    "        # if train_steps % checkpoint_freq == 0:\n",
    "        #     checkpoint_path = os.path.join(checkpoint_dir, f\"lora_step_{train_steps}.pt\")\n",
    "        #     # Save LoRA adapter weights\n",
    "        #     model_lora.save_pretrained(checkpoint_path)\n",
    "        #     # Save optimizer state\n",
    "        #     torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, f\"optimizer_step_{train_steps}.pt\"))\n",
    "        #     print(f\"Checkpoint saved at step {train_steps}\")\n",
    "\n",
    "    \n",
    "        ### FOR EVERY TRAINING-STEP WE RUN V-B BATCH 0(T_B * V_B)\n",
    "        # Validation Loop\n",
    "\n",
    "        model_lora.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_input_ids, batch_target_ids in val_loader:\n",
    "                val_op = model_lora(batch_input_ids, labels=batch_target_ids)\n",
    "                val_losses.append(val_op.loss.cpu().item())\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_curve.append(avg_val_loss)\n",
    "            \n",
    "            # # Save best model based on validation loss\n",
    "            # if avg_val_loss < best_val_loss:\n",
    "            #     best_val_loss = avg_val_loss\n",
    "            #     best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "            #     model_lora.save_pretrained(best_model_path)\n",
    "            #     print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        if train_steps >= target_steps:  # Stop training at the required steps\n",
    "            break\n",
    "        \n",
    "        model_lora.train()  # Resume training mode\n",
    "    \n",
    "# # Save final model\n",
    "# final_model_path = os.path.join(checkpoint_dir, \"final_model.pt\")\n",
    "# model_lora.save_pretrained(final_model_path)\n",
    "\n",
    "lt = time.time()\n",
    "print('Time taken:', (lt - ft) / 60, 'mins')\n",
    "\n",
    "model_lora.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_curve)), train_curve, color = 'red', marker = '.', label = 'Train')\n",
    "plt.plot(range(len(val_curve)), val_curve, color = 'blue', marker = '.', label = 'Validation')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('#Optimization Steps')\n",
    "\n",
    "plt.title('Loss-Curve')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ft = time.time()\n",
    "# torch.save(model, f\"saves/model_{target_steps}_{train_split}.pth\")\n",
    "# lt = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('time-taken: ', (lt - ft)/60, ' mins')  Loading Model\n",
    " # model_list = os.listdir('saves')\n",
    "# path = os.path.join('saves', model_list[0])\n",
    "# print('Load-File: ', path)\n",
    "\n",
    "# model_lora = torch.load(path, weights_only=False)\n",
    "# model_lora.eval()  NOTE: Performing Forecasting HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = test_encoded\n",
    "print('Test-Prompt: ')\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = time.time()\n",
    "prey_pred_response = generate_forecast_v2(model_lora, test_prompt, tokenizer, inf_max_new_tokens=manual_config['inf_max_tokens'])\n",
    "# prey_pred_response = generate_forecast(model_lora, test_prompt, tokenizer, max_new_tokens=max_tokens)\n",
    "\n",
    "lt = time.time()\n",
    "\n",
    "print('time-taken: ', (lt - ft) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(prey_pred_response), len(prey_pred_response))\n",
    "print(prey_pred_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_decoded_response, pred_decoded_response = extract_forecasts(prey_pred_response)\n",
    "\n",
    "print(len(prey_decoded_response), len(pred_decoded_response))\n",
    "print(prey_decoded_response)\n",
    "\n",
    "prey_decoded_response = ts_decoding(prey_decoded_response, model_type=\"llama\", precision=3, offsets=prey_os['offset'][check_rn], scale_factors=prey_os['scale'][check_rn])[:forecast_length]\n",
    "pred_decoded_response = ts_decoding(pred_decoded_response, model_type=\"llama\", precision=3, offsets=pred_os['offset'][check_rn], scale_factors=pred_os['scale'][check_rn])[:forecast_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fig, axs = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "axs[0].plot(time_data_past, data_prey[check_rn].tolist(), label = 'Past Data')\n",
    "axs[0].plot(time_data_true[:len(prey_decoded_response)], prey_decoded_response, label = 'Prediction', marker = '.')\n",
    "axs[0].plot(time_data_true, data_prey_true[check_rn].tolist(), label = 'Truth', marker = '.')\n",
    "\n",
    "axs[0].set_title('Prey-Population (Joint)')\n",
    "axs[0].set_xlabel('time')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(time_data_past, data_pred[check_rn].tolist(), label = 'Past Data')\n",
    "axs[1].plot(time_data_true[:len(prey_decoded_response)], pred_decoded_response, label = 'Prediction', marker = '.')\n",
    "axs[1].plot(time_data_true, data_pred_true[check_rn].tolist(), label = 'Truth', marker = '.')\n",
    "\n",
    "axs[1].set_title('Predator-Population (Joint)')\n",
    "axs[1].set_xlabel('time')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "Jointly show them in the same plot \n",
    "'''\n",
    "print()\n",
    "\n",
    "plt.plot(time_data_past, data_prey[check_rn].tolist(), label = 'Past Data')\n",
    "plt.plot(time_data_true, prey_decoded_response, label = 'Prediction', marker = '.')\n",
    "plt.plot(time_data_true, data_prey_true[check_rn].tolist(), label = 'Truth', marker = '.')\n",
    "\n",
    "plt.plot(time_data_past, data_pred[check_rn].tolist(), label = 'Past Data')\n",
    "plt.plot(time_data_true, pred_decoded_response, label = 'Prediction', marker = '.')\n",
    "plt.plot(time_data_true, data_pred_true[check_rn].tolist(), label = 'Truth', marker = '.')\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.title('Prey-Predator-Population (Joint)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
